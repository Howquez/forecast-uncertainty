# 🌦 What makes a (weather) forecast credible?
[![Generic badge](https://img.shields.io/badge/Status:-WIP-yellow.svg)](https://shields.io/)
Made with [oTree](https://www.sciencedirect.com/science/article/pii/S2214635016000101) and ❤️

## 🧐 What the project is about
This project is the foundation of an economic experiment that exposes respondents to forecasts and manipulates the 
forecasts' communication strategies between subjects. We elicit the respondents' 
ambiguity attitudes (Baillon) before and after exposure to assess forecasts' credibility.
The ambiguity attitudes are elicited using Baillon et al.'s [(2018, Econometrica)]( https://doi.org/10.3982/ECTA14370) 
Method. 


## 🚏 How you can access a demo
You can find the experiment's demo [here](https://forecastsurvey.herokuapp.com/demo/). A click on the app will create a session and redirect you to a page containing lots of URLs. Click on the
_Session-wide link_ to open the experiment. After reading through the instructions, you should end up seeing a decision 
screen looking like this one:

[![](figures/Baillon_Decision_Screen.png)](https://forecastsurvey.herokuapp.com/demo/)

## 🎓 Academic Contributions
TBD

## 📊 How we analyzed the data 
The analysis plan will be pre-registered, which means that we declare what we are analyzing and how we are analyzing it 
before seeing the data.

📖 Read the docs: 
I am creating a wiki [over here](https://github.com/Howquez/forecast-uncertainty/wiki). It will contain more detailed 
information needed to understand the resulting data and to replicate the analysis.

## 🛠 How we built it

### Tech stack
The experiment itself is based on [oTree](https://www.sciencedirect.com/science/article/pii/S2214635016000101), 
a Python module designed to build surveys and experiments. It utilizes 
Python, JavaScript, HTML & CSS (mostly bootstrap 4.1.x). The corresponding analysis is done with R.

## ✅ To do
[Here](https://github.com/Howquez/forecast-uncertainty/projects/1) is the corresponding kanban board.



